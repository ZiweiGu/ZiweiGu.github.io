I"à<p>First, some clarifications. We are considering linear maps from a finite-dimensional vector space to itself (also called an <strong>operator</strong> L(V)), as opposed to linear maps from one vector space to another vector space.</p>

<p>Suppose <em>V</em> is a finite-dimensional vector space, <em>T</em> âˆˆ <em>L</em>(<em>V</em>), and Î» âˆˆ <em>F</em>.</p>

<p>Some definitions to know:</p>
<ul>
  <li>
    <p>Invariant subspace: U is invariant under T if T|<sub>U</sub> is an operator on U.</p>
  </li>
  <li>
    <p>Î» âˆˆ <em>F</em>  is an eigenvalue of T if âˆƒ v âˆˆ V s.t. v â‰  0 and Tv = Î»v.</p>
  </li>
  <li>
    <p>The eigenspace of T corresponding to Î», E(Î», T) = null(T - Î»I) (the set of all eigenvectors of T corresponding to Î», along with the 0 vector.)</p>
  </li>
  <li>
    <p>Concepts from <a href="/2019/08/17/jectivity.html">a previous post</a></p>
  </li>
</ul>

<p><strong>Basics</strong></p>

<ul>
  <li>
    <p>A linear map is invertible if and only if it is injective and surjective.</p>
  </li>
  <li>
    <p>For operators on a finite-dimensional vector space, however, either injectivity or surjectivity alone implies the other condition, and thus implies invertibility. (proved using Rank theorem)</p>
  </li>
  <li>
    <p>If Î» is an eigenvalue of T, then by definition v âˆˆ ker(T - Î»I) so ker(T - Î»I) â‰  0. Hence, T - Î»I is not injective (or surjective). That is, T - Î»I is not invertible, det(T - Î»I) = 0.</p>
  </li>
  <li>
    <p>Eigenvectors corresponding to <strong>distinct</strong> eigenvalues are linearly independent.</p>
  </li>
  <li>
    <p>Each operator on V has at most dim V distinct eigenvalues.</p>
  </li>
  <li>
    <p>If A is an n Ã—n matrix, then the sum of the n eigenvalues of A is the trace of A and the
product of the n eigenvalues is the determinant of A.</p>
  </li>
  <li>
    <p>If Î» is an eigenvalue of the T, Î»<sup>2</sup> is an eigenvalue of T<sup>2</sup>.</p>
  </li>
  <li>
    <p>n Ã— n matrix A and its transpose A<sup>T</sup> have the same eigenvalues.</p>
  </li>
  <li>
    <p>Suppose A and B are similar matrices. Then A and B have the same characteristic polynomial and
hence the same eigenvalue.</p>
  </li>
  <li>
    <p>If Î» is an eigenvalue of A, then the dimension of E<sub>Î»</sub> is at most the multiplicity of Î».</p>
  </li>
  <li>
    <p>If Î»<sup>*</sup> is an eigenvalue of A, then the multiplicity of Î»<sup>*</sup> is at least the dimension of the eigenspace E<sub>Î»<sup>*</sup></sub> .</p>
  </li>
</ul>

<p><strong>Intermediate</strong></p>

<ul>
  <li>
    <p>Any two polynomials of an operator commute.</p>
  </li>
  <li>
    <p>Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue.</p>
  </li>
  <li>
    <p>Over C, every operator has an upper-triangular matrix</p>
  </li>
  <li>
    <p>Suppose T âˆˆ L(V) has an upper-triangular matrix with respect to some basis of V, then T is invertible if and only if all the entries on the diagonal of that upper-triangular matrix are nonzero;</p>
  </li>
  <li>
    <p>Suppose T âˆˆ L(V) has an upper-triangular matrix with respect to some basis of V, then the eigenvalues of T are precisely the entries on the diagonal of that upper-triangular matrix.</p>
  </li>
  <li>
    <p>Let Î»<sub>1</sub>, â€¦, Î»<sub>m</sub> denote the distinct eigenvalues of T. Then, T is diagonalizable</p>
  </li>
</ul>

<p>Â Â Â Â Â Â Â Â  &lt;=&gt; V has a basis consisting of eigenvectors of T</p>

<p>Â Â Â Â Â Â Â Â  &lt;=&gt; âˆƒ 1-dimensional subspaces U<sub>1</sub>, â€¦, U<sub>n</sub> of V, each invariant under T, such that V Â Â Â Â Â Â Â Â  = U<sub>1</sub> âŠ• â€¦ âŠ• U<sub>n</sub></p>

<p>Â Â Â Â Â Â Â Â  &lt;=&gt; V = E(Î»<sub>1</sub>, T) âŠ• â€¦ âŠ• E(Î»<sub>m</sub>, T)</p>

<p>Â Â Â Â Â Â Â Â  &lt;=&gt; dim V = dim E(Î»<sub>1</sub>, T) + â€¦ + dim E(Î»<sub>m</sub>, T)</p>

<ul>
  <li>If T has dim V (enough) distinct eigenvalues, then T is diagonalizable. (The converse is not true, as the diagonalizable identity operator only has only 1 eigenvalue Î» = 1)</li>
</ul>

:ET