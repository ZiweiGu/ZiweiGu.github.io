<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-10-31T15:51:42+08:00</updated><id>/feed.xml</id><entry><title type="html">Why Eigenvalues Are So Important</title><link href="/math%20notes/2019/08/19/why-eigenvalues.html" rel="alternate" type="text/html" title="Why Eigenvalues Are So Important" /><published>2019-08-19T00:00:00+08:00</published><updated>2019-08-19T00:00:00+08:00</updated><id>/math%20notes/2019/08/19/why-eigenvalues</id><content type="html" xml:base="/math%20notes/2019/08/19/why-eigenvalues.html">&lt;p&gt;Okay, let’s bring out the question most professors would not bother to explain: why do we care so much about vectors that become a multiple of themselves when you multiply by a matrix?&lt;/p&gt;

&lt;p&gt;Short (and very shallow) answer: Applications.&lt;/p&gt;

&lt;p&gt;Eigenvalues are essential in physical applications like rotations, image compression, and quantum mechanics, and numerous data science applications (PCA, markov chains, latent variable models, and much more…) They are not only used to explain natural occurrences, but also to discover new and better designs for the future.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A longer and deeper answer&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;“A central goal of linear algebra is to show that given an operator T ∈ L(V), there exists a basis of V with respect to which T has a reasonably simple matrix” (e.g. with many 0’s - diagonal matrix). Diagonal matrices are nice because it’s easier to take powers, decompose, visualize, etc.&lt;/p&gt;

&lt;p&gt;Furthermore, eigenvalues offer you the power to distinguish among representations. A matrix is simply one way of representing a linear transformation in a particular basis you chose, but the matrix changes when you change the basis. Eigenvalues, on the other hand, are invariant under change of basis.” Therefore, they are much more fundamental. They characterize the intrinsic properties about a linear transformation. Math, in some sense, is to find those things intrinsic to the universe. It’s therefore not surprising that eigenvalues are everywhere.&lt;/p&gt;</content><author><name>Ziwei Gu</name></author><category term="Math Notes" /><category term="Linear Algebra" /><summary type="html">Okay, let’s bring out the question most professors would not bother to explain: why do we care so much about vectors that become a multiple of themselves when you multiply by a matrix?</summary></entry><entry><title type="html">Facts about Eigenvalues</title><link href="/math%20notes/2019/08/18/facts-about-eigenvalues.html" rel="alternate" type="text/html" title="Facts about Eigenvalues" /><published>2019-08-18T00:00:00+08:00</published><updated>2019-08-18T00:00:00+08:00</updated><id>/math%20notes/2019/08/18/facts-about-eigenvalues</id><content type="html" xml:base="/math%20notes/2019/08/18/facts-about-eigenvalues.html">&lt;p&gt;First, some clarifications. We are considering linear maps from a finite-dimensional vector space to itself (also called an &lt;strong&gt;operator&lt;/strong&gt; L(V)), as opposed to linear maps from one vector space to another vector space.&lt;/p&gt;

&lt;p&gt;Suppose &lt;em&gt;V&lt;/em&gt; is a finite-dimensional vector space, &lt;em&gt;T&lt;/em&gt; ∈ &lt;em&gt;L&lt;/em&gt;(&lt;em&gt;V&lt;/em&gt;), and λ ∈ &lt;em&gt;F&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Some definitions to know:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Invariant subspace: U is invariant under T if T|&lt;sub&gt;U&lt;/sub&gt; is an operator on U.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;λ ∈ &lt;em&gt;F&lt;/em&gt;  is an eigenvalue of T if ∃ v ∈ V s.t. v ≠ 0 and Tv = λv.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The eigenspace of T corresponding to λ, E(λ, T) = null(T - λI) (the set of all eigenvectors of T corresponding to λ, along with the 0 vector.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Concepts from &lt;a href=&quot;/math%20notes/2019/08/17/jectivity.html&quot;&gt;a previous post&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Basics&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A linear map is invertible if and only if it is injective and surjective.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For operators on a finite-dimensional vector space, however, either injectivity or surjectivity alone implies the other condition, and thus implies invertibility. (proved using Rank theorem)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If λ is an eigenvalue of T, then by definition v ∈ ker(T - λI) so ker(T - λI) ≠ 0. Hence, T - λI is not injective (or surjective). That is, T - λI is not invertible, det(T - λI) = 0.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Eigenvectors corresponding to &lt;strong&gt;distinct&lt;/strong&gt; eigenvalues are linearly independent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each operator on V has at most dim V distinct eigenvalues.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If A is an n ×n matrix, then the sum of the n eigenvalues of A is the trace of A and the
product of the n eigenvalues is the determinant of A.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If λ is an eigenvalue of the T, λ&lt;sup&gt;2&lt;/sup&gt; is an eigenvalue of T&lt;sup&gt;2&lt;/sup&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;n × n matrix A and its transpose A&lt;sup&gt;T&lt;/sup&gt; have the same eigenvalues.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Suppose A and B are similar matrices. Then A and B have the same characteristic polynomial and
hence the same eigenvalue.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If λ is an eigenvalue of A, then the dimension of E&lt;sub&gt;λ&lt;/sub&gt; is at most the multiplicity of λ.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If λ&lt;sup&gt;*&lt;/sup&gt; is an eigenvalue of A, then the multiplicity of λ&lt;sup&gt;*&lt;/sup&gt; is at least the dimension of the eigenspace E&lt;sub&gt;λ&lt;sup&gt;*&lt;/sup&gt;&lt;/sub&gt; .&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Intermediate&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Any two polynomials of an operator commute.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Over C, every operator has an upper-triangular matrix&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Suppose T ∈ L(V) has an upper-triangular matrix with respect to some basis of V, then T is invertible if and only if all the entries on the diagonal of that upper-triangular matrix are nonzero;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Suppose T ∈ L(V) has an upper-triangular matrix with respect to some basis of V, then the eigenvalues of T are precisely the entries on the diagonal of that upper-triangular matrix.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let λ&lt;sub&gt;1&lt;/sub&gt;, …, λ&lt;sub&gt;m&lt;/sub&gt; denote the distinct eigenvalues of T. Then, T is diagonalizable&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;         &amp;lt;=&amp;gt; V has a basis consisting of eigenvectors of T&lt;/p&gt;

&lt;p&gt;         &amp;lt;=&amp;gt; ∃ 1-dimensional subspaces U&lt;sub&gt;1&lt;/sub&gt;, …, U&lt;sub&gt;n&lt;/sub&gt; of V, each invariant under T, such that V          = U&lt;sub&gt;1&lt;/sub&gt; ⊕ … ⊕ U&lt;sub&gt;n&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;         &amp;lt;=&amp;gt; V = E(λ&lt;sub&gt;1&lt;/sub&gt;, T) ⊕ … ⊕ E(λ&lt;sub&gt;m&lt;/sub&gt;, T)&lt;/p&gt;

&lt;p&gt;         &amp;lt;=&amp;gt; dim V = dim E(λ&lt;sub&gt;1&lt;/sub&gt;, T) + … + dim E(λ&lt;sub&gt;m&lt;/sub&gt;, T)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If T has dim V (enough) distinct eigenvalues, then T is diagonalizable. (The converse is not true, as the diagonalizable identity operator only has only 1 eigenvalue λ = 1)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ziwei Gu</name></author><category term="Math Notes" /><category term="Linear Algebra" /><summary type="html">First, some clarifications. We are considering linear maps from a finite-dimensional vector space to itself (also called an operator L(V)), as opposed to linear maps from one vector space to another vector space.</summary></entry><entry><title type="html">Let’s finally get those “-jectivity”s right</title><link href="/math%20notes/2019/08/17/jectivity.html" rel="alternate" type="text/html" title="Let’s finally get those “-jectivity”s right" /><published>2019-08-17T00:00:00+08:00</published><updated>2019-08-17T00:00:00+08:00</updated><id>/math%20notes/2019/08/17/jectivity</id><content type="html" xml:base="/math%20notes/2019/08/17/jectivity.html">&lt;p&gt;Definitions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;null space&lt;/strong&gt; of T: null T = {v ∈ V: Tv = 0} (stuff in the original vector space that’s mapped to 0 by T)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;range&lt;/strong&gt; of T: range T = {Tv: v ∈ V} (all the resulting vectors after the mapping T)&lt;/li&gt;
  &lt;li&gt;A function T: V -&amp;gt; W is &lt;strong&gt;injective&lt;/strong&gt; if Tu = Tv implies u = v.&lt;/li&gt;
  &lt;li&gt;A function T: V -&amp;gt; W is &lt;strong&gt;surjective&lt;/strong&gt; if its range equals W.&lt;/li&gt;
  &lt;li&gt;An &lt;strong&gt;isomorphism&lt;/strong&gt; is an invertible (injective + surjective) linear map.&lt;/li&gt;
  &lt;li&gt;Two vector spaces are isomorphic if there is an isomorphism from one vector space onto the other one.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Facts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A linear map is injective iff null T = {0}.
&lt;span style=&quot;color:gray&quot;&gt;[Proof Idea: if v ∈ null T then T(v) = 0 = T(0) =&amp;gt; v = 0 so null T ⊆ {0}; other direction trivial.]&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rank-Nullity: if T ∈ L(V, W) then dim V = dim null T + dim range T&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A map to a smaller dimensional space is not injective. (“smaller” as measured by &lt;em&gt;dimension&lt;/em&gt;)
&lt;span style=&quot;color:gray&quot;&gt;[Proof Idea: using Rank-Nullity]&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A map to a larger dimensional space is not surjective.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If T is invertible, it’s inverse is unique.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two finite-dimensional vector spaces over F are isomorphic if and only if they have the same dimension. 
&lt;span style=&quot;color:gray&quot;&gt;[Proof Idea: consider T: V -&amp;gt; W; =&amp;gt;: using Rank-Nullity; &amp;lt;=: denote basis of V, W. Apply T to the linear combination of the basis of V, which results in the same linear combination of the basis of W, we can show T is surjective and injective, and is thus invertible.]&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;L(V, W) and F&lt;sup&gt;m,n&lt;/sup&gt; are isomorphic&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;dim F&lt;sup&gt;m,n&lt;/sup&gt; = mn 
&lt;span style=&quot;color:gray&quot;&gt;[Proof Idea: m-by-n matrices that have 0 in all entries except for a 1 in one entry form a basis of F&lt;sup&gt;m,n&lt;/sup&gt;. How many of them are there?]&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Based on the previous two facts, dim L(V, W) = (dim V)(dim W)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ziwei Gu</name></author><category term="Math Notes" /><category term="Linear Algebra" /><summary type="html">Definitions:</summary></entry></feed>